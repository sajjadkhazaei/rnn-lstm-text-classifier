üì∞ AG News Text Classifier
This project implements a deep learning‚Äìbased text classification model using RNN / LSTM (or GRU) to classify news articles from the AG News dataset into four categories.
The goal of this project is to demonstrate a complete NLP pipeline, from dataset loading and preprocessing to model training, evaluation, and inference ‚Äî all inside a single Jupyter Notebook.
________________________________________
üìå Project Overview

‚Ä¢	Dataset: AG News
‚Ä¢	Task: Multi-class text classification
‚Ä¢	Classes:
1.	World üåç
2.	Sports üèÖ
3.	Business üíº
4.	Sci/Tech üî¨

‚Ä¢	Tokenizer: BERT Tokenizer (bert-base-uncased)
‚Ä¢	Models Supported:
‚Ä¢	RNN
‚Ä¢	LSTM
‚Ä¢	GRU
‚Ä¢	Framework: PyTorch
________________________________________
üìÇ Notebook Structure
1Ô∏è‚É£ Imports & Setup

‚Ä¢	Install required libraries (datasets, transformers, torchmetrics)
‚Ä¢	Import Python and PyTorch modules
‚Ä¢	Select device (cuda / cpu)
________________________________________
2Ô∏è‚É£ Dataset & Tokenizer

‚Ä¢	Load the AG News dataset using datasets.load_dataset
‚Ä¢	Define BertTokenizerFast
‚Ä¢	Tokenization, padding, and truncation
‚Ä¢	Set PyTorch format (input_ids, attention_mask, label)
‚Ä¢	Create DataLoaders for training and testing
________________________________________
3Ô∏è‚É£ Utils

‚Ä¢	Define AverageMeter to compute average loss
‚Ä¢	Additional helper utilities (e.g., accuracy tracking)
________________________________________
4Ô∏è‚É£ Model Definition

‚Ä¢	Define RNNModel class supporting RNN / LSTM / GRU
‚Ä¢	Architecture:
‚Ä¢	Embedding layer
‚Ä¢	Recurrent layer
‚Ä¢	Fully connected classification layer
________________________________________
5Ô∏è‚É£ Training Function

‚Ä¢	train_one_epoch function
‚Ä¢	Batch-wise training loop
‚Ä¢	Loss and accuracy computation
________________________________________
6Ô∏è‚É£ Validation Function

‚Ä¢	Validation loop without backpropagation
‚Ä¢	Computes validation loss and accuracy
________________________________________
7Ô∏è‚É£ Training Loop

‚Ä¢	Loop over multiple epochs
‚Ä¢	Save the best model using torch.save
‚Ä¢	Store training and validation metrics for visualization
________________________________________
8Ô∏è‚É£ Plot Results

‚Ä¢	Plot:
‚Ä¢	Training vs Validation Loss
‚Ä¢	Training vs Validation Accuracy
________________________________________
9Ô∏è‚É£ Inference

‚Ä¢	Perform predictions on:
‚Ä¢	Custom input sentences
‚Ä¢	Batches from the test set
________________________________________
üöÄ Features

‚Ä¢	Tokenization using BERT tokenizer
‚Ä¢	Supports RNN, LSTM, and GRU
‚Ä¢	Optional bidirectional RNNs
‚Ä¢	Proper handling of padding and truncation
‚Ä¢	End-to-end training, evaluation, and inference
‚Ä¢	Clean and educational notebook-style implementation
________________________________________
üìö Installation

Clone the repository and install dependencies:
git clone <your-repo-url>
cd rnn-lstm-text-classifier
pip install -r requirements.txt
________________________________________
üì¶ Requirements

torch
torchvision
torchtext
transformers
datasets
torchmetrics
tqdm
matplotlib
________________________________________
üìä Example Inference

Text: "Apple releases a new iPhone model"
Prediction: Business
________________________________________
üéØ Purpose of This Project

This project is designed to:
‚Ä¢	Practice NLP with deep learning
‚Ä¢	Understand RNN/LSTM behavior on text
‚Ä¢	Build a portfolio-ready project
‚Ä¢	Prepare for ML / DL interviews and real-world tasks

